# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Load dataset
df = pd.read_csv("your_dataset.csv")  # Update with actual file path

# Display first few rows
print(df.head())

# ---------------------- 1. Data Preprocessing ---------------------- #

# Drop unnecessary columns (if any)
df = df.drop(columns=['Coordinates', 'Information Source'])  # Modify based on dataset

# Check for missing values
print(df.isnull().sum())

# Fill missing values
num_imputer = SimpleImputer(strategy="median")  # Fill numerical with median
cat_imputer = SimpleImputer(strategy="most_frequent")  # Fill categorical with most common value

# Define categorical & numerical features
categorical_features = ['Region of Origin', 'Region of Incident', 'Country of Origin', 'Migration Route']
numerical_features = ['Incident Year', 'Reported Month', 'Number of Dead', 'Total Number of Dead and Missing',
                      'Number of Survivors', 'Number of Females', 'Number of Males', 'Number of Children']

# One-hot encode categorical features
preprocessor = ColumnTransformer([
    ('num', num_imputer, numerical_features),
    ('cat', OneHotEncoder(handle_unknown="ignore"), categorical_features)
])

# Apply transformation
df_transformed = preprocessor.fit_transform(df)

# Convert to DataFrame
df_transformed = pd.DataFrame(df_transformed)

# ---------------------- 2. Train-Test Split ---------------------- #
X = df_transformed
y = df['Target Variable']  # 0 = Non-Violent, 1 = Violent

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ---------------------- 3. Model Training ---------------------- #

# Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# ---------------------- 4. Model Evaluation ---------------------- #

# Predictions
log_pred = log_reg.predict(X_test)
rf_pred = rf.predict(X_test)

# Evaluation Metrics
print("Logistic Regression Performance:")
print(classification_report(y_test, log_pred))

print("Random Forest Performance:")
print(classification_report(y_test, rf_pred))

# ---------------------- 5. Hyperparameter Tuning ---------------------- #

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20]
}

# Grid Search for best Random Forest parameters
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best Hyperparameters:", grid_search.best_params_)

# Train final model with best parameters
best_rf = RandomForestClassifier(**grid_search.best_params_, random_state=42)
best_rf.fit(X_train, y_train)

# Final Evaluation
final_pred = best_rf.predict(X_test)
print("Final Model Performance:")
print(classification_report(y_test, final_pred))

# ---------------------- 6. Feature Importance ---------------------- #

# Get feature importances from best model
feature_importances = best_rf.feature_importances_

# Convert to DataFrame
feature_df = pd.DataFrame({"Feature": X_train.columns, "Importance": feature_importances})
feature_df = feature_df.sort_values(by="Importance", ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_df["Importance"], y=feature_df["Feature"])
plt.title("Feature Importance in Random Forest")
plt.show()
